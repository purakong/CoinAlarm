"""
ê±°ë˜ëŸ‰ ê¸‰ì¦ ìŠ¤ìº” ë¡œì§

ëª¨ë“  USDT ì‹¬ë³¼ì˜ ë°ì´í„°ë¥¼ ìµœì‹ í™”í•˜ê³  ê±°ë˜ëŸ‰ ê¸‰ì¦ì„ í™•ì¸
"""
import sys,os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import os
import logging
from datetime import datetime, timedelta
from pytz import timezone
from core.downloader import ChartDownloader
from service.filter import Filter
from core.scheduler_state import scheduler_info


class SurgeScanner:
    """ê±°ë˜ëŸ‰ ê¸‰ì¦ ìŠ¤ìºë„ˆ"""
    
    def __init__(self, db_config, result_file="surge_results.json", history_file="surge_history.json", config_file="config.json"):
        """
        Args:
            db_config: DB ì—°ê²° ì •ë³´
            result_file: ìµœì‹  ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…
            history_file: ì´ë ¥ ì €ì¥ íŒŒì¼ëª…
            config_file: ì„¤ì • íŒŒì¼ëª…
        """
        self.db_config = db_config
        self.result_file = result_file
        self.history_file = history_file
        self.config_file = config_file
        self.config = self._load_config()
        self.latest_results = {
            "last_update": None,
            "surge_coins": []
        }
        
        # ë¡œê±° ì„¤ì •
        self.logger = self._setup_logger()
    
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            if not os.path.exists(self.config_file):
                self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼({self.config_file})ì´ ì—†ìŠµë‹ˆë‹¤.")
                self.logger.critical("ì„¤ì • íŒŒì¼ì„ ì‘ì„±í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
                exit()
            
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # í•„ìˆ˜ í‚¤ ê²€ì¦
            required_keys = {
                'scanner': ['symbol_limit', 'batch_size', 'batch_delay', 'keep_candles'],
                'tot_timeframes': None,  # ë¦¬ìŠ¤íŠ¸ íƒ€ì…
                'filter': None  # ë¦¬ìŠ¤íŠ¸ íƒ€ì…, í•˜ìœ„ ê²€ì¦ í•„ìš”
            }
            
            # ìµœìƒìœ„ í‚¤ ê²€ì¦
            for key in required_keys.keys():
                if key not in config:
                    self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ì— í•„ìˆ˜ í‚¤ '{key}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
                    exit()
            
            # scanner í•˜ìœ„ í‚¤ ê²€ì¦
            for sub_key in required_keys['scanner']:
                if sub_key not in config['scanner']:
                    self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ì˜ 'scanner'ì— í•„ìˆ˜ í‚¤ '{sub_key}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
                    exit()
            
            # filter ë°°ì—´ ê²€ì¦
            if not isinstance(config['filter'], list) or len(config['filter']) == 0:
                self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ì˜ 'filter'ëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
                exit()
            
            # ê° í•„í„° ì„¤ì • ê²€ì¦
            filter_required_keys = ['types', 'using_timeframe', 'interval', 'period', 'window']
            for i, filter_config in enumerate(config['filter']):
                for key in filter_required_keys:
                    if key not in filter_config:
                        self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ì˜ filter[{i}]ì— í•„ìˆ˜ í‚¤ '{key}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
                        exit()
            
            return config

        except json.JSONDecodeError as e:
            self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ JSON í˜•ì‹ ì˜¤ë¥˜: {e}")
            self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
            exit()
        except Exception as e:
            self.logger.critical(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.logger.critical("ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì‹œì˜¤!!. í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
            exit()
            
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('SurgeScanner')
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        if logger.handlers:
            logger.handlers.clear()
        
        # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
        log_level = self.config.get('logging', {}).get('level', 'INFO')
        logger.setLevel(getattr(logging, log_level))
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(getattr(logging, log_level))
        
        # í¬ë§· ì„¤ì •
        log_format = self.config.get('logging', {}).get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        formatter = logging.Formatter(log_format)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
        return logger
    
    def _get_current_time(self):
        return datetime.now()
    
    def scan(self):
        """
        ì „ì²´ ìŠ¤ìº” ì‹¤í–‰
        1. ë°ì´í„° ìµœì‹ í™”
        2. ê±°ë˜ëŸ‰ ê¸‰ì¦ í•„í„°ë§
        3. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        4. ê²°ê³¼ ì €ì¥
        """
        self.logger.info("="*50)
        self.logger.info(f"ê±°ë˜ëŸ‰ ê¸‰ì¦ ìŠ¤ìº” ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("="*50)
        
        # ë‹¤ìš´ë¡œë”ì™€ í•„í„° ìƒì„±
        downloader = ChartDownloader(self.db_config)
        filter_obj = Filter()  # DB ì˜ì¡´ì„± ì œê±°
        
        # ì„¤ì •ì—ì„œ limit ê°’ ê°€ì ¸ì˜¤ê¸°
        symbol_limit = self.config.get('scanner').get('symbol_limit')
        
        # ëª¨ë“  USDT ì‹¬ë³¼ ê°€ì ¸ì˜¤ê¸°
        all_symbols = downloader.get_all_usdt_symbols(limit=symbol_limit)
        self.logger.info(f"ì´ {len(all_symbols)}ê°œ ì‹¬ë³¼ í™•ì¸ ì¤‘ (ì„¤ì •: {symbol_limit if symbol_limit else 'ì „ì²´'})")
        
        # í™•ì¸í•  ì‹œê°„ë´‰ë“¤ (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        timeframes = self.config.get('tot_timeframes')
        
        # 1ë‹¨ê³„: ë°ì´í„° ìµœì‹ í™”
        self._update_data(downloader, all_symbols, timeframes)
        
        # 2ë‹¨ê³„: ê±°ë˜ëŸ‰ ê¸‰ì¦ í•„í„°ë§
        surge_data = self.apply_filter(filter_obj, all_symbols)
        
        # 3ë‹¨ê³„: ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        self._cleanup_old_data(downloader)
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(surge_data)
        
        # ì—°ê²° ì¢…ë£Œ
        downloader.close()
        
        self.logger.info(f"ìŠ¤ìº” ì™„ë£Œ! ê²°ê³¼ê°€ {self.result_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _update_data(self, downloader:ChartDownloader, symbols, timeframes):
        """
        ëª¨ë“  ì‹¬ë³¼ì˜ ë°ì´í„° ìµœì‹ í™”
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ API ì œí•œ ë°©ì§€
        """
        self.logger.info("ë°ì´í„° ìµœì‹ í™” ì‹œì‘")
        update_count = 0
        batch_size = self.config.get('scanner', {}).get('batch_size', 10)
        batch_delay = self.config.get('scanner', {}).get('batch_delay', 1)
        
        for timeframe in timeframes:
            self.logger.info(f"{timeframe} ì‹œê°„ë´‰ ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            # ì‹¬ë³¼ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            total_batches = (len(symbols) - 1) // batch_size + 1
            for i in range(0, len(symbols), batch_size):
                batch = symbols[i:i+batch_size]
                batch_num = i // batch_size + 1
                
                self.logger.info(f"{timeframe} ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ì‹¬ë³¼)")
                
                for symbol in batch:
                    try:
                        downloader.download_and_save(symbol, timeframe, initial_limit=350)
                        update_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"{symbol} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                # ë°°ì¹˜ ê°„ ë”œë ˆì´ (API ì œí•œ ë°©ì§€)
                import time
                time.sleep(batch_delay)
        
        self.logger.info(f"ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {update_count}ê°œ)")

    def _check_filter_scheduling(self, filter_configs):
        """
        í•„í„° ìŠ¤ì¼€ì¤„ë§ í™•ì¸ ë° íŠ¸ë¦¬ê±° ì„¤ì •
        
        Args:
            filter_configs: í•„í„° ì„¤ì • ë¦¬ìŠ¤íŠ¸
        """
        current_time = self._get_current_time()
        
        for filter_config in filter_configs:
            filter_type = filter_config.get('types')
            interval = filter_config.get('interval')
            filter_enable = filter_config.get('enable')
            
            if filter_type is None or interval is None:
                self.logger.warning(f"âš ï¸ í•„í„° ì„¤ì • ì˜¤ë¥˜: 'types' ë˜ëŠ” 'interval' ëˆ„ë½")
                continue
            
            if filter_enable is False:
                self.logger.info(f"â¸ï¸ í•„í„° '{filter_type}' ë¹„í™œì„±í™”ë¨, ìŠ¤ì¼€ì¤„ë§ ê±´ë„ˆëœ€")
                continue
            
            # interval íŒŒì‹±
            try:
                if interval.endswith('m'):
                    minutes = int(interval.replace('m', ''))
                elif interval.endswith('h'):
                    minutes = int(interval.replace('h', '')) * 60
                elif interval.endswith('d'):
                    minutes = int(interval.replace('d', '')) * 1440
                else:
                    self.logger.warning(f"âš ï¸ í•„í„° '{filter_type}': interval í˜•ì‹ ì˜¤ë¥˜ ('{interval}'). 'm', 'h', 'd' ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                    continue
            except ValueError:
                self.logger.warning(f"âš ï¸ í•„í„° '{filter_type}': interval ê°’ ë³€í™˜ ì‹¤íŒ¨ ('{interval}')")
                continue
            
            # scheduler_infoì— í•„í„° íƒ€ì…ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            if filter_type not in scheduler_info:
                scheduler_info[filter_type] = {
                    'start_time': None,
                    'elapsed_time': timedelta(0),
                    'trigger': False
                }
                self.logger.info(f"ğŸ†• í•„í„° '{filter_type}' ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”")
            
            # start_timeì´ Noneì´ë©´ ìµœì´ˆ ì‹¤í–‰
            if scheduler_info[filter_type]['start_time'] is None:
                scheduler_info[filter_type]['start_time'] = current_time
                scheduler_info[filter_type]['trigger'] = True
                self.logger.info(f"âœ… í•„í„° '{filter_type}' ìµœì´ˆ ì‹¤í–‰ íŠ¸ë¦¬ê±° (interval: {interval})")
            else:
                # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
                elapsed_time = current_time - scheduler_info[filter_type]['start_time']
                scheduler_info[filter_type]['elapsed_time'] = elapsed_time
                
                # interval ì´ìƒ ê²½ê³¼í–ˆìœ¼ë©´ íŠ¸ë¦¬ê±°
                if elapsed_time >= timedelta(minutes=minutes):
                    scheduler_info[filter_type]['start_time'] = current_time
                    scheduler_info[filter_type]['trigger'] = True
                    self.logger.info(f"âœ… í•„í„° '{filter_type}' íŠ¸ë¦¬ê±° (ê²½ê³¼: {int(elapsed_time.total_seconds()/60)}ë¶„, interval: {interval})")
                else:
                    scheduler_info[filter_type]['trigger'] = False
                    remaining = timedelta(minutes=minutes) - elapsed_time
                    self.logger.info(f"â­ï¸  í•„í„° '{filter_type}' ëŒ€ê¸° ì¤‘ (ë‚¨ì€ ì‹œê°„: {int(remaining.total_seconds()/60)}ë¶„/{interval})")
    def apply_filter(self, filter_obj:Filter, symbols):
        """
        ê±°ë˜ëŸ‰ ê¸‰ì¦ í•„í„°ë§
        """
        surge_data = []
        
        # ì„¤ì •ì—ì„œ í•„í„° ë°°ì—´ ê°€ì ¸ì˜¤ê¸°
        filter_configs = self.config.get('filter', [])
        
        # í•˜ìœ„ í˜¸í™˜ì„±: filterê°€ dictë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(filter_configs, dict):
            filter_configs = [filter_configs]
        
        # í•„í„° ì´ë¦„ë“¤ ì¶œë ¥
        filter_names = [f.get('types', 'unknown') for f in filter_configs]
        self.logger.info(f"ğŸ” ì‚¬ìš© ì¤‘ì¸ í•„í„°: {', '.join(filter_names)}")
        
        downloader = ChartDownloader(self.db_config)
            
        surge_symbols = []
        
        # í•„í„° ìŠ¤ì¼€ì¤„ë§ í™•ì¸
        self._check_filter_scheduling(filter_configs)
        
        # íŠ¸ë¦¬ê±°ëœ í•„í„°ë“¤ í™•ì¸
        triggered_filters = {}
        for filter_config in filter_configs:
            filter_type = filter_config.get('types')
            if filter_type in scheduler_info:
                if scheduler_info[filter_type].get('trigger', False):
                    triggered_filters[filter_type] = filter_config
        
        if not triggered_filters:
            self.logger.info("íŠ¸ë¦¬ê±°ëœ í•„í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í•„í„°ê°€ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.")
            downloader.close()
            return surge_data
        
        self.logger.info(f"íŠ¸ë¦¬ê±°ëœ í•„í„°: {', '.join(triggered_filters.keys())}")
        
        # ê° ì‹¬ë³¼ë³„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ì— ì£¼ì…
        for symbol in symbols:
            try:
                # íŠ¸ë¦¬ê±°ëœ í•„í„°ë“¤ë§Œ ì‹¤í–‰
                for filter_type, filter_config in triggered_filters.items():
                    
                    if filter_type == '3step_surge':
                        # í•´ë‹¹ í•„í„° ì„¤ì • ê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
                        filter_timeframes:list = filter_config.get('using_timeframe')
                        volume_range_multiplier = filter_config.get('volume_range_multiplier')
                        period = filter_config.get('period')
                        window = filter_config.get('window')
                        range_multiplier = filter_config.get('range_multiplier')
                        strong_candle_count = filter_config.get('strong_candle_count', 0)
                        upper_wick_ratio = filter_config.get('upper_wick_ratio', 0.2)
                        lower_wick_ratio = filter_config.get('lower_wick_ratio', 0.1)

                        for timeframe in filter_timeframes:
                            candles = downloader.db.get_candles(symbol, timeframe, limit=window + period + 1)
                            pattern_time = filter_obj._three_step_surge_filter(
                                candles, symbol, volume_range_multiplier, period, window, range_multiplier,
                                strong_candle_count=strong_candle_count,
                                upper_wick_ratio=upper_wick_ratio,
                                lower_wick_ratio=lower_wick_ratio
                            )
                            if pattern_time:
                                surge_symbols.append({
                                    "symbol": symbol, 
                                    "time": pattern_time, 
                                    "filter": filter_type,
                                    "timeframe": timeframe
                                })
                                break  # í•˜ë‚˜ì˜ í•„í„°ì— ê±¸ë¦¬ë©´ ë‹¤ìŒ ì‹¬ë³¼ë¡œ
                    
                    elif filter_type == 'high_volume_spike':
                        # í•´ë‹¹ í•„í„° ì„¤ì • ê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
                        filter_timeframes:list = filter_config.get('using_timeframe')
                        period = filter_config.get('period')
                        window = filter_config.get('window')
                        volume_range_multiplier = filter_config.get('volume_range_multiplier')
                        spike_threshold = filter_config.get('spike_threshold')
                        
                        for timeframe in filter_timeframes:
                            candles = downloader.db.get_candles(symbol, timeframe, limit=window + period + 1)
                            pattern_time = filter_obj._high_volume_spike_filter(candles, symbol, downloader=downloader, timeframe=timeframe, period=period, window=window, volume_range_multiplier=volume_range_multiplier, spike_threshold=spike_threshold)
                            if pattern_time:
                                surge_symbols.append({
                                    "symbol": symbol, 
                                    "time": pattern_time, 
                                    "filter": filter_type,
                                    "timeframe": timeframe
                                })
                                break  # í•˜ë‚˜ì˜ í•„í„°ì— ê±¸ë¦¬ë©´ ë‹¤ìŒ ì‹¬ë³¼ë¡œ
            
            except Exception as e:
                self.logger.warning(f"âš ï¸ {symbol} í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # í•„í„° ì‹¤í–‰ ì™„ë£Œ í›„ triggerë¥¼ Falseë¡œ ì„¤ì •
        for filter_type in triggered_filters.keys():
            scheduler_info[filter_type]['trigger'] = False
            self.logger.debug(f"í•„í„° '{filter_type}' ì‹¤í–‰ ì™„ë£Œ, trigger=False ì„¤ì •")
        
        if surge_symbols:
            self.logger.info(f"ğŸ”¥ ì´ {len(surge_symbols)}ê°œ ì‹¬ë³¼ ë°œê²¬")
            
            # ì‹œê°€ì´ì•¡ ì •ë³´ ì¶”ê°€
            self.logger.info(f"ğŸ’° ì‹œê°€ì´ì•¡ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            for symbol_info in surge_symbols:
                try:
                    market_cap = downloader.get_market_cap(symbol_info['symbol'])
                    symbol_info['market_cap'] = market_cap
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {symbol_info['symbol']} ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    symbol_info['market_cap'] = None
            
            # timeframeë³„ë¡œ ê·¸ë£¹í™”
            timeframe_groups = {}
            for symbol_info in surge_symbols:
                tf = symbol_info['timeframe']
                if tf not in timeframe_groups:
                    timeframe_groups[tf] = []
                timeframe_groups[tf].append(symbol_info)
            
            # surge_data ìƒì„±
            for tf, symbols_list in timeframe_groups.items():
                surge_data.append({
                    "timeframe": tf,
                    "count": len(symbols_list),
                    "symbols": symbols_list
                })
                self.logger.info(f"ğŸ”¥ {tf}: {len(symbols_list)}ê°œ ë°œê²¬")
        
        downloader.close()
        return surge_data
    
    def _cleanup_old_data(self, downloader: ChartDownloader):
        """
        ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        ê° ì‹¬ë³¼/ì‹œê°„ë´‰ë‹¹ ìµœì‹  Nê°œë§Œ ìœ ì§€
        """
        self.logger.info(f"\nğŸ—‘ï¸ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        keep_count = self.config.get('scanner', {}).get('keep_candles', 10000)
        deleted = downloader.db.cleanup_all_old_data(keep_count=keep_count)
        if deleted > 0:
            self.logger.info(f"âœ… {deleted}ê°œ ì˜¤ë˜ëœ ìº”ë“¤ ì‚­ì œ ì™„ë£Œ")
        else:
            self.logger.info(f"âœ… ì •ë¦¬í•  ë°ì´í„° ì—†ìŒ")
    
    def _save_results(self, surge_data):
        """
        ê²°ê³¼ ì €ì¥
        - surge_results.json: ìµœì‹  ê²°ê³¼ (ë®ì–´ì“°ê¸°)
        - surge_history.json: ì „ì²´ ì´ë ¥ (ì¶”ê°€)
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ìµœì‹  ê²°ê³¼
        self.latest_results = {
            "last_update": timestamp,
            "surge_coins": surge_data
        }
        
        # ìµœì‹  ê²°ê³¼ íŒŒì¼ì— ì €ì¥ (ë®ì–´ì“°ê¸°)
        with open(self.result_file, 'w', encoding='utf-8') as f:
            json.dump(self.latest_results, f, ensure_ascii=False, indent=2)
        
        # ì´ë ¥ íŒŒì¼ì— ì¶”ê°€ (surge_coinsê°€ ìˆì„ ë•Œë§Œ)
        try:
            # surge_coinsì— ì‹¤ì œ ë°œê²¬ëœ ì½”ì¸ì´ ìˆëŠ”ì§€ í™•ì¸
            has_surge = False
            for item in surge_data:
                if item.get('symbols') and len(item['symbols']) > 0:
                    has_surge = True
                    break
            
            # ë°œê²¬ëœ ì½”ì¸ì´ ì—†ìœ¼ë©´ ì´ë ¥ì— ì €ì¥í•˜ì§€ ì•ŠìŒ
            if not has_surge:
                return
            
            # ê¸°ì¡´ ì´ë ¥ ë¡œë“œ
            if os.path.exists(self.history_file):
                try:
                    with open(self.history_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            history = json.loads(content)
                        else:
                            history = {"scans": []}
                except (json.JSONDecodeError, ValueError):
                    self.logger.warning("ì´ë ¥ íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    history = {"scans": []}
            else:
                history = {"scans": []}
            
            # ìƒˆ ìŠ¤ìº” ê²°ê³¼ ì¶”ê°€
            scan_result = {
                "timestamp": timestamp,
                "surge_coins": surge_data
            }
            history["scans"].append(scan_result)
            
            # ì´ë ¥ ì €ì¥ (ìµœëŒ€ 300ê°œë§Œ ìœ ì§€)
            max_history = 300
            if len(history["scans"]) > max_history:
                history["scans"] = history["scans"][-max_history:]
            
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“ ìŠ¤ìº” ì´ë ¥ ì €ì¥ ì™„ë£Œ (ì´ {len(history['scans'])}ê°œ)")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_latest_results(self):
        """
        ìµœì‹  ê²°ê³¼ ë°˜í™˜
        """
        try:
            with open(self.result_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return self.latest_results
    
    def get_history(self, limit=10):
        """
        ìŠ¤ìº” ì´ë ¥ ë°˜í™˜
        
        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10, Noneì´ë©´ ì „ì²´)
        
        Returns:
            ì´ë ¥ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
                scans = history.get("scans", [])
                
                # ìµœì‹ ìˆœ ì •ë ¬ (ì—­ìˆœ)
                scans.reverse()
                
                if limit:
                    return scans[:limit]
                return scans
        except:
            return []
        
if __name__ == "__main__":
    # DB ì„¤ì • (ë³¸ì¸ ì„¤ì •ì— ë§ê²Œ ìˆ˜ì •)
    DB_CONFIG = {
        'host': 'localhost',
        'user': 'root',
        'password': '1234',
        'database': 'coin_chart'
    }

    # ìŠ¤ìºë„ˆ ìƒì„±
    scanner = SurgeScanner(DB_CONFIG, result_file="data/surge_results.json", history_file="data/surge_history.json")
    downloader = ChartDownloader()
    filter_obj = Filter()
    symbols = ['GRIFFAINUSDT']
    timeframe = '1m'
    volume_range_multiplier = 5
    range_multiplier = 3
    period = 14
    window = 30 
    candles = downloader.get_candles_by_time_range('1000LUNCUSDT', '5m', '2025-12-04 0:20:00', '2025-12-05 05:00:00', timezone='KST')
    pattern_time = filter_obj._three_step_surge_filter(candles, '1000LUNCUSDT', volume_range_multiplier, period, window, range_multiplier, start_time='2025-12-04 22:00:00', end_time='2025-12-05 03:00:00', timezone='KST')
    if pattern_time:
        print(f"íŒ¨í„´ ë°œê²¬ ì‹œê°„: {pattern_time}")

